---
abstract: Abbreviation disambiguation is important for automated clinical note processing
  due to the frequent use of abbreviations in clinical settings. Current models for
  automated abbreviation disambiguation are restricted by the scarcity and imbalance
  of labeled training data, decreasing their generalizability to orthogonal sources.
  In this work we propose a novel data augmentation technique that utilizes information
  from related medical concepts, which improves our model’s ability to generalize.
  Furthermore, we show that incorporating the global context information within the
  whole medical note (in addition to the traditional local context window), can significantly
  improve the model’s representation for abbreviations. We train our model on a public
  dataset (MIMIC III) and test its performance on datasets from different sources
  (CASI, i2b2). Together, these two techniques boost the accuracy of abbreviation
  disambiguation by almost 14{%} on the CASI dataset and 4{%} on i2b2.
title: 'Training without training data: Improving the generalizability of automated
  medical abbreviation disambiguation'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: skreta20a
month: 0
tex_title: "{Training without training data: Improving the generalizability of automated
  medical abbreviation disambiguation}"
firstpage: 233
lastpage: 245
page: 233-245
order: 233
cycles: false
bibtex_author: Skreta, Marta and Arbabi, Aryan and Wang, Jixuan and Brudno, Michael
author:
- given: Marta
  family: Skreta
- given: Aryan
  family: Arbabi
- given: Jixuan
  family: Wang
- given: Michael
  family: Brudno
date: 2020-04-30
address: 
publisher: PMLR
container-title: Proceedings of the Machine Learning for Health NeurIPS Workshop
volume: '116'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 4
  - 30
pdf: http://proceedings.mlr.press/v116/skreta20a/skreta20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
